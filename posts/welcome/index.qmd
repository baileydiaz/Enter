---
title: "BYUI Education Project"
author: "Bailey Diaz"
date: "2024-05-29"
categories: [news]
---

<!-- ```{r setup, include=FALSE} 
knitr::opts_chunk$set(warning = FALSE, message = FALSE, echo = FALSE) 
``` -->

For the past few weeks I have been working with Benjamin Pacini on his doctoral dissertation.  His topic of interest is, what makes a good teacher?  

To start off, I decided to create a model that predicts Danielson scores based off of Disposition scores.  Danielson scores are the scores that evaluators give student teachers.  They evaluate how well they do at controlling the classroom, teaching ability, and so fourth.  

What we are using to predict Danielson scores are called dispositions.  Dispositions are scores that professors give their students.  These are essentially judgements calls and ask questions such as, "Does this person seem to enjoy being with children?"

So, my initial analysis investigated the following train of thought, do these judgement calls (dispositions) seem to have a relation to the actual student teacher evaluations? 

To clarify, the dispositions are done when the student teacher is not present.  

```{r}
library(arrow)
library(ggplot2)
library(tidyverse)
library(pander)
library(car)
library(readxl)
library(dplyr)

library(sandwich)
library(lmtest)

```


```{r}
# Read the Parquet file
demos <- read_parquet("C:/Users/19518/Desktop/byui_education_project/Data/demographics.parquet")

teacher_data <- read_parquet("C:/Users/19518/Desktop/byui_education_project/Data/All_teacher_info.parquet")

combined = merge(demos, teacher_data, by = "StudentID")

no_na <- combined %>%
  filter(!is.na(Danielson_492)) %>% 
  filter(!is.na(Disposition_400))

#no_na <- no_na[, c("StudentID", "Danielson_492", "Disposition_400")]

no_na <- no_na %>%
  group_by(StudentID) %>%
  slice(1) %>%
  ungroup()

```

## Danielson Predictor

These are the results of a very simple regression and the r-squared does not imply that we have strong correlation.  However, in social sciences we do not expect to see very high r-squared values.  In all honesty, this r-squared is higher than any other regression that we have tried.  

```{r}
## This appears to be interesting.  
mylm <- lm(Danielson_492 ~ Disposition_highest_level, data = no_na)

pander(summary(mylm))

```


# Future Endeavors

## Robust Standard Errors
The chair for Brother Pacini's research has mentioned the importance of residual standard errors. I need to investigate the importance of this metric and see how it can apply to our research. 
```{r}
pander(coeftest(mylm, vcov = vcovHC(mylm, type="HC1")))
```


## Graph

```{r}
b <- coef(mylm)

b

ggplot(teacher_data, aes(x = Disposition_400, y = Danielson_492)) +
  geom_point() +
  geom_smooth(method = "loess", se = FALSE, color = "red")+
  stat_function(fun = function(x) b[1] + b[2]*x, color="blue") 
```
 On the graph we have a lowess curve along with the line that our regression created.  Lowess curves essentially creates local averages and graphs those.  On some level it shows us the "ideal" curve.  So, the "ideal" is blue and the current is red.  We are very close to the ideal, so what else can be done?

## Checking Requirements

Then, I decided to check the normality requirements for the regression. If these diagnostic plots are not within normal bounds then that leads us to doubt any discoveries made due to our analysis.  The biggest concern for me is the second plot.  The data points are supposed to be on a line.  You can see that they deviate from that line.  

```{r}

#boxCox(mylm)

par(mfrow=c(1,3))
plot(mylm,which=1:2)
plot(mylm$residuals)
```

## Transformations

The data I used for this analysis utilizes z-scores instead of the raw data.  Transformations could possibly help with the normality of the data and give more validity to our research. The current state of the data does not allow me to use transformations. As a result, I have begun to wrangle the raw data so that I can perform transformations and gather cleaner more valuable and reliable insights from the data.   